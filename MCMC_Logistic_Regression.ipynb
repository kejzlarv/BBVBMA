{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "broke-acquisition",
   "metadata": {},
   "source": [
    "# Package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "qualified-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from time import time\n",
    "import copy as cp\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import seaborn\n",
    "import theano as thno\n",
    "import theano.tensor as T\n",
    "import itertools as itr\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy import integrate\n",
    "from scipy.optimize import fmin_powell\n",
    "from scipy.stats import binom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-computer",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incorporate-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "### log transformation of variables in the hear disease data\n",
    "def log_transform(data, center = False, scale = False):\n",
    "    data.loc[:, data.columns == 'age'] = np.log(data.loc[:, data.columns == 'age'])\n",
    "    data.loc[:, data.columns == 'trestbps'] = np.log(data.loc[:, data.columns == 'trestbps'])\n",
    "    data.loc[:, data.columns == 'chol'] = np.log(data.loc[:, data.columns == 'chol'])\n",
    "    data.loc[:, data.columns == 'thalach'] = np.log(data.loc[:, data.columns == 'thalach'])\n",
    "    data.loc[:, data.columns == 'oldpeak'] = np.log(data.loc[:, data.columns == 'oldpeak'])\n",
    "    if center:\n",
    "        data.loc[:, data.columns == 'age'] = data.loc[:, data.columns == 'age']-  data.loc[:, data.columns == 'age'].mean()\n",
    "        data.loc[:, data.columns == 'trestbps'] = data.loc[:, data.columns == 'trestbps'] - data.loc[:, data.columns == 'trestbps'].mean()\n",
    "        data.loc[:, data.columns == 'chol'] = data.loc[:, data.columns == 'chol'] - data.loc[:, data.columns == 'chol'].mean()\n",
    "        data.loc[:, data.columns == 'thalach'] = data.loc[:, data.columns == 'thalach'] - data.loc[:, data.columns == 'thalach'].mean()\n",
    "        data.loc[:, data.columns == 'oldpeak'] = data.loc[:, data.columns == 'oldpeak'] - data.loc[:, data.columns == 'oldpeak'].mean()\n",
    "    if scale:\n",
    "        data.loc[:, data.columns != 'target'] = data.loc[:, data.columns != 'target'] / data.loc[:, data.columns != 'target'].std() \n",
    "    return data\n",
    "\n",
    "def power_set_index(n_predictors):\n",
    "    \"\"\"Generates a matrix of variable indicators defining the space of \n",
    "    all the models\n",
    "    \n",
    "    Args:\n",
    "        n_predictors: Number of predictors to be considered\n",
    "    Returns:\n",
    "        A matrix with variable indicators\"\"\"\n",
    "    power_set  = itr.product([0,1], repeat = n_predictors)\n",
    "    array_index = []\n",
    "    for i in list(power_set):\n",
    "        array_index = array_index + [np.array(i)]\n",
    "    array_index = np.array(array_index)\n",
    "    ids = np.array([i for i in range(len(array_index))])\n",
    "    return np.append( np.append(ids[:,None], np.ones(len(array_index))[:, None], axis = 1), array_index, axis = 1)\n",
    "\n",
    "def evidence_integral(n_mc, n_mc_evidence, data_evidence, sigma):\n",
    "    \"\"\"MC approximation of the evidence integral\"\"\"\n",
    "    log_evidence = np.zeros(n_mc)\n",
    "    # likelihood eval\n",
    "    for i in tqdm(range(n_mc), desc = \"log likelihood eval\"):\n",
    "        np.random.normal(0, sigma, len(variables))\n",
    "        z = np.sum(data_evidence * np.random.normal(0, sigma, len(variables)), axis = 1)\n",
    "        p = 1 / (1 + np.exp(- z))\n",
    "        log_evidence[i] = np.sum(binom.logpmf(k=data[\"target\"].values, n=1, p = p))\n",
    "\n",
    "    mc_integral = np.empty(n_mc_evidence)\n",
    "    for i in tqdm(range(n_mc_evidence), desc = \"Integral calc\"):\n",
    "        j = (i + n_mc - n_mc_evidence)\n",
    "        m = max(log_evidence[:(j + 1)])\n",
    "        mc_integral[i] = (np.exp(m) * np.sum(np.exp(log_evidence[:(j + 1)] - m))) / (j + 1)\n",
    "    return log_evidence, mc_integral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-bulgarian",
   "metadata": {},
   "source": [
    "# MCMC psoterior approximation and MC evidence computaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "centered-bottom",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using adapt_diag...\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [Intercept]\n",
      "Sampling chain 0, 0 divergences: 100%|█████████████████████████████████████████| 30000/30000 [00:19<00:00, 1572.10it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "log likelihood eval: 100%|███████████████████████████████████████████████████| 100000/100000 [00:19<00:00, 5179.75it/s]\n",
      "Integral calc: 100%|█████████████████████████████████████████████████████████████| 10000/10000 [01:53<00:00, 88.32it/s]\n"
     ]
    }
   ],
   "source": [
    "### Loading and pre-processing data\n",
    "np.random.seed(0)\n",
    "heart = pd.read_csv(\"heart.csv\", index_col= None)\n",
    "\n",
    "data_raw = cp.deepcopy(heart[~pd.isnull(heart[\"target\"])])\n",
    "variables_set = [\"target\", \"chol\", \"trestbps\", \"sex\"]\n",
    "K = len(variables_set) - 1 # Number of predictors\n",
    "model_space = power_set_index(K).astype(int)\n",
    "\n",
    "### MCMC approximation and MC integration for a single model\n",
    "model_index = 0\n",
    "for model in [model_index]:\n",
    "    variables = np.array(variables_set)[model_space[model,1:].astype(bool)]\n",
    "    data = cp.deepcopy(log_transform(data_raw[variables], center = True))\n",
    "\n",
    "    # MCMC\n",
    "    sigma = float(3.0) # Standard deviation for the prior distribution of regression params\n",
    "    n_sample = 20000\n",
    "    n_tune = 10000\n",
    "    sampler = \"NUTS\"\n",
    "    model_string = \"target ~ 1\"\n",
    "    for i in range(len(variables) - 1):\n",
    "        model_string = model_string + '+' + variables[i+1]\n",
    "        \n",
    "    with pm.Model() as logistic_model:\n",
    "        priors = {\n",
    "            \"Intercept\": pm.Normal.dist(mu = 0, sigma = sigma)\n",
    "        }\n",
    "        for var in variables[1:]:\n",
    "            priors[var] = pm.Normal.dist(mu = 0, sigma = sigma)\n",
    "\n",
    "        pm.glm.GLM.from_formula(\n",
    "            model_string, data, family=pm.glm.families.Binomial(), priors=priors\n",
    "        )\n",
    "        if sampler == \"NUTS\":\n",
    "            trace = pm.sample(n_sample, tune=n_tune, init=\"adapt_diag\", chains = 1)\n",
    "        elif sampler == \"MH\":\n",
    "            step = pm.Metropolis()\n",
    "            trace = pm.sample(50000, tune=5000, chains = 1, step = step)\n",
    "        pm.backends.ndarray.save_trace(trace, \"trace_logistic_\" + str(n_tune) + \"_\" + str(n_sample) + \"_sigma_\" + str(sigma) + \"_model_\" + str(model) + \"_sampler_\" + sampler, overwrite=True)\n",
    "\n",
    "    # MC    \n",
    "    n_mc = 100000 # Number of MC samples used for evidence computation\n",
    "    n_mc_evidence = 10000 # Number of samples stored\n",
    "    data_evidence = cp.deepcopy(data.values)\n",
    "    data_evidence[:,0] = np.ones(len(data))\n",
    "    log_evidence, mc_integral = evidence_integral(n_mc, n_mc_evidence, data_evidence, sigma)\n",
    "    evidence_results = {\"log_evidence\": log_evidence, \"mc_integral\": mc_integral}\n",
    "    with open(\"evidence_sigma_\" + str(sigma) + \"_model_\" + str(model) + \"_nmc_\" + str(n_mc) + '.pickle', 'wb') as handle:\n",
    "        pkl.dump(evidence_results , handle, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
